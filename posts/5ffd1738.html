<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">
  








<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="算法,深度学习,特征工程," />










<meta name="description" content="主要是搬运《Feature Engineering for Machine Learning》在 Apache.cn上的翻译版本中的第四、五、六章，依旧会根据自己的理解进行小小的修改。 第四章：特征缩放的效果：从词袋到TF—IDF 第五章：类别特征：机器鸡时代的鸡蛋计数 第六章：降维：使用PCA压缩数据集">
<meta name="keywords" content="算法,深度学习,特征工程">
<meta property="og:type" content="article">
<meta property="og:title" content="特征工程(中)">
<meta property="og:url" content="https://www.shelhon.cn/posts/5ffd1738.html">
<meta property="og:site_name" content="无问西东">
<meta property="og:description" content="主要是搬运《Feature Engineering for Machine Learning》在 Apache.cn上的翻译版本中的第四、五、六章，依旧会根据自己的理解进行小小的修改。 第四章：特征缩放的效果：从词袋到TF—IDF 第五章：类别特征：机器鸡时代的鸡蛋计数 第六章：降维：使用PCA压缩数据集">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/1.png">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/2.png">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/3.png">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/4.png">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/5.png">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/6.jpg">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/7.jpg">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/8.png">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/9.png">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/10.png">
<meta property="og:image" content="https://www.shelhon.cn/posts/5ffd1738/11.png">
<meta property="og:updated_time" content="2019-05-24T09:28:29.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="特征工程(中)">
<meta name="twitter:description" content="主要是搬运《Feature Engineering for Machine Learning》在 Apache.cn上的翻译版本中的第四、五、六章，依旧会根据自己的理解进行小小的修改。 第四章：特征缩放的效果：从词袋到TF—IDF 第五章：类别特征：机器鸡时代的鸡蛋计数 第六章：降维：使用PCA压缩数据集">
<meta name="twitter:image" content="https://www.shelhon.cn/posts/5ffd1738/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.shelhon.cn/posts/5ffd1738.html"/>





  <title>特征工程(中) | 无问西东</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3275656b1c327868bf311aa7c5fdabd2";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
	
	<a href="https://github.com/Shelhon/Shelhon.github.io" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#64CEAA; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">无问西东</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">糊涂脸水聪明枕</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.shelhon.cn/posts/5ffd1738.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qsx">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="无问西东">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">特征工程(中)</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-14T09:58:19+08:00">
                2019-05-14
              </time>
            

            

            
          </span>

    
    
        
            <span class="post-updated">
            &nbsp; | &nbsp; 更新于
            <time itemprop="dateUpdated" datetime="2019-05-24T17:28:29+08:00" content="2019-05-24">
                2019-05-24
            </time>
            </span>
        
		  
		 
		 
          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  12,287
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  44
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>主要是搬运《Feature Engineering for Machine Learning》在 Apache.cn上的翻译版本中的第四、五、六章，依旧会根据自己的理解进行小小的修改。</p>
<p>第四章：特征缩放的效果：从词袋到TF—IDF</p>
<p>第五章：类别特征：机器鸡时代的鸡蛋计数</p>
<p>第六章：降维：使用PCA压缩数据集</p>
<a id="more"></a>
<hr>
<h1 id="特征缩放的效果：从词袋到TF—IDF"><a href="#特征缩放的效果：从词袋到TF—IDF" class="headerlink" title="特征缩放的效果：从词袋到TF—IDF"></a>特征缩放的效果：从词袋到TF—IDF</h1><p>字袋易于生成，但远非完美。假设我们平等的统计所有单词，有些不需要的词也会被强调。在第三章提过一个例子，Emma and the raven。我们希望在文档表示中能强调两个主要角色。示例中，“Eama”和“raven”都出现了3词，但是“the”的出现高达8次，“and”出现了5次，另外“it”以及“was”也都出现了4词。仅仅通过简单的频率统计，两个主要角色并不突出。这是有问题的。</p>
<p>其他的像是“magnificently,” “gleamed,” “intimidated,” “tentatively,” 和“reigned,”这些辅助奠定段落基调的词也是很好的选择。它们表示情绪，这对数据科学家来说可能是非常有价值的信息。 所以，理想情况下，我们会倾向突出对有意义单词的表示。</p>
<h2 id="Tf-Idf-词袋的小转折"><a href="#Tf-Idf-词袋的小转折" class="headerlink" title="Tf-Idf:词袋的小转折"></a>Tf-Idf:词袋的小转折</h2><p>Tf-Idf 是词袋的一个小小的转折。它表示词频-逆文档频。</p>
<p>tf-idf不是查看每个文档中每个单词的原始计数，而是查看每个单词计数除以出现该单词的文档数量的标准化计数。</p>
<p>bow(w,d) = #times word w appears in document d</p>
<script type="math/tex; mode=display">
tf-idf(w,d)=\frac{\operatorname{bow}(w, d) \times N}{(\# \text { documents in which word w appears)}}</script><p>其中N代表数据集中所有文档的数量。</p>
<p>分数$\frac{\operatorname{bow}(w, d) \times N}{(# \text { documents in which word w appears)}}$就是所谓的逆文件频率。</p>
<p>如果一个单词出现在许多文档中，则其逆文档频率接近1。</p>
<p>如果出现在较少文档中，则逆文档频率要高的多。</p>
<p>或者，对原始逆文档频率进行对数转换（<strong>自然对数</strong>，即以e为底），可以将1变为0，并使得较大的数字（比1大得多）变小。</p>
<p>如果将tf-idf定义为：</p>
<script type="math/tex; mode=display">
t f-i d f(w, d)=\frac{\operatorname{bow}(w, d) \times N}{(\# \text { documents in which word } w \text { appears) }}</script><p>那么每个文档中出现一次的单词都将被有效清零，并且只出现在少数文档中的单词的计数将比以前更大。</p>
<script type="math/tex; mode=display">
t f-i d f(w, d)=\frac{\operatorname 文档d中出现单词w的次数 \times 总文档数}{\ \text { 出现过单词 } w \text { 的文档数量 } }</script><script type="math/tex; mode=display">
t f-i d f(w, d)=文档d中出现单词w的次数 \times ln(\frac{\operatorname 总文档数}{\ \text { 出现过单词 } w \text { 的文档数量 } })</script><p>看图片来了解它的具体内容。下图展示了一个包含4个句子的简单样例：“it is a puppy,” “it is a cat,” “it is a kitten,” 以及 “that is a dog and this is a pen.” 我们将这些句子绘制在“puppy”，“cat”以及“is”三个词的特征空间上。</p>
<img src="/posts/5ffd1738/1.png">
<p>现在让我们看看对逆文档频进行对数变换之后，相同四个句子的tf-idf表示。 下图显示了相应特征空间中的文档。可以注意到，单词“is”被有效地消除，因为它出现在该数据集中的所有句子中。另外，单词“puppy”和“cat”都只出现在四个句子中的一个句子中，所以现在这两个词计数得比之前更高（ln(4)=1.38…&gt;1）。因此tf-idf使罕见词语更加突出，并有效地忽略了常见词汇。它与<a href="https://www.shelhon.cn/posts/8ae97bf8.html#%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%EF%BC%9A%E5%B1%95%E5%BC%80%E3%80%81%E8%BF%87%E6%BB%A4%E5%92%8C%E5%88%86%E5%9D%97">第三章</a>中基于频率的滤波方法密切相关，但比放置严格截止阈值更具数学优雅性。</p>
<img src="/posts/5ffd1738/2.png">
<h2 id="Tf-Idf的含义"><a href="#Tf-Idf的含义" class="headerlink" title="Tf-Idf的含义"></a>Tf-Idf的含义</h2><p>Tf-idf使罕见的单词更加突出，并有效地忽略了常见单词。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>Tf-idf 通过乘以一个常量来转换字数统计特性。因此，它是特征缩放的一个例子，这是<a href="https://www.shelhon.cn/posts/8ae97bf8.html#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E6%88%96%E5%BD%92%E4%B8%80%E5%8C%96">第2章</a>介绍的一个概念。特征缩放在实践中效果有多好？ </p>
<p>具体去原文的<a href="[http://fe4ml.apachecn.org/#/docs/4.%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E7%9A%84%E6%95%88%E6%9E%9C%EF%BC%9A%E4%BB%8E%E8%AF%8D%E8%A2%8B%E5%88%B0_TF-IDF](http://fe4ml.apachecn.org/#/docs/4.特征缩放的效果：从词袋到_TF-IDF">第四章</a>)样例 4里看。</p>
<h3 id="首先建立分类数据集"><a href="#首先建立分类数据集" class="headerlink" title="首先建立分类数据集"></a>首先建立分类数据集</h3><p>在区分餐厅和夜生活场所的时候，发现两个类别之间的评论数目有很大的差异，这是因为类不平衡数据集。</p>
<p>对于构建模型来说，不平衡的数据集存在一个问题：这个模型会把大部分精力花在比重更大的类上。</p>
<p>针对这种问题，并且在两个类别上都有大量的数据，那么可以用一个较好的方法，是将数目较大的类（餐厅）进行下采样，使之与数目较小的类（夜生活）数目大致相同。</p>
<h3 id="用tf-idf-转换缩放词袋"><a href="#用tf-idf-转换缩放词袋" class="headerlink" title="用tf-idf 转换缩放词袋"></a>用tf-idf 转换缩放词袋</h3><p>这个实验的目标是比较词袋，tf-idf以及L2归一化对于线性分类的作用。注意，做tf-idf接着做L2归一化和单独做L2归一化是一样的。所以我们需要只需要3个特征集合：词袋，tf-idf，以及逐词进行L2归一化后的词袋。</p>
<p>在这个例子中，我们将使用Scikit-learn的CountVectorizer将评论文本转化为词袋。所有的文本特征化方法都依赖于标记器（tokenizer），该标记器能够将文本字符串转换为标记（词）列表。在这个例子中，Scikit-learn的默认标记模式是查找2个或更多字母数字字符的序列。标点符号被视为标记分隔符。</p>
<h3 id="测试集上进行特征缩放"><a href="#测试集上进行特征缩放" class="headerlink" title="测试集上进行特征缩放"></a>测试集上进行特征缩放</h3><p>特征缩放的一个细微之处是它需要了解我们在实践中很可能不知道的特征统计，例如均值，方差，文档频率，L2范数等。为了计算tf-idf表示，我们不得不根据训练数据计算逆文档频率，并使用这些统计量来调整训练和测试数据。在Scikit-learn中，将特征变换拟合到训练集上相当于收集相关统计数据。然后可以将拟合过的变换应用于测试数据。</p>
<p>当我们使用训练统计来衡量测试数据时，结果看起来有点模糊。测试集上的最小-最大比例缩放不再整齐地映射到零和一。L2范数，平均数和方差统计数据都将显得有些偏离。这比缺少数据的问题好一点。例如，测试集可能包含训练数据中不存在的单词，并且对于新的单词没有相应的文档频。通常的解决方案是简单地将测试集中新的单词丢弃。这似乎是不负责任的，但训练集上的模型在任何情况下都不会知道如何处理新词。一种稍微不太好的方法是明确地学习一个“垃圾”单词，并将所有罕见的频率单词映射到它，即使在训练集中也是如此，正如“罕见词汇”中所讨论的那样。</p>
<h2 id="使用逻辑回归进行分类"><a href="#使用逻辑回归进行分类" class="headerlink" title="使用逻辑回归进行分类"></a>使用逻辑回归进行分类</h2><p>逻辑回归是一个简单的线性分类器。通过对输入特征的加权组合，输入到一个sigmoid函数。sigmoid函数将任何实数平滑的映射到介于0和1之间。如下图绘制sigmoid函数曲线。由于逻辑回归比较简单，因此它通常是最先接触的分类器。</p>
<img src="/posts/5ffd1738/3.png">
<p>该图是sigmoid函数的插图。该函数将输入的实数x转换为一个0到1之间的数。它有一组参数w，表示围绕中点0.5增加的斜率。截距项b表示函数输出穿过中点的输入值。如果sigmoid输出大于0.5，则逻辑分类器将预测为正例，否则为反例。通过改变w和b，可以控制决策的改变，以及决策响应该点周围输入值变化的速度。</p>
<p>在使用默认参数训练逻辑回归分类器时得结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Test score with bow features: 0.775873066497</span><br><span class="line"># Test score with l2-normalized features: 0.763514590974</span><br><span class="line"># Test score with tf-idf features: 0.743182905438</span><br></pre></td></tr></table></figure>
<p>矛盾的是，结果表明最准确的分类器是使用BOW特征的分类器。出乎意料我们之外。事实证明，造成这种情况的原因是没有很好地“调整”分类器，这是比较分类器时一个常见的错误。</p>
<h2 id="使用正则化调整逻辑回归"><a href="#使用正则化调整逻辑回归" class="headerlink" title="使用正则化调整逻辑回归"></a>使用正则化调整逻辑回归</h2><p>逻辑回归有些华而不实。 当特征的数量大于数据点的数量时，找到最佳模型的问题被认为是欠定的。 解决这个问题的一种方法是在训练过程中增加额外的约束条件。 这就是所谓的正则化，技术细节将在下一节讨论。</p>
<p>逻辑回归的大多数实现允许正则化。为了使用这个功能，必须指定一个正则化参数。正则化参数是在模型训练过程中未自动学习的超参数。相反，他们必须手动进行调整，并将其提供给训练算法。这个过程称为超参数调整。调整超参数的一种基本方法称为网格搜索：指定一个超参数值网格，并且调谐器以编程方式在网格中搜索最佳超参数设置格。 找到最佳超参数设置后，使用该设置对整个训练集进行训练，并比较测试集上这些同类最佳模型的性能。</p>
<h2 id="重点：比较模型时调整超参数"><a href="#重点：比较模型时调整超参数" class="headerlink" title="重点：比较模型时调整超参数"></a>重点：比较模型时调整超参数</h2><p>比较模型或特征时，调整超参数非常重要。 软件包的默认设置将始终返回一个模型。 但是除非软件在底层进行自动调整，否则很可能会返回一个基于次优超参数设置的次优模型。 分类器性能对超参数设置的敏感性取决于模型和训练数据的分布。 逻辑回归对超参数设置相对稳健（或不敏感）。 即便如此，仍然有必要找到并使用正确的超参数范围。 否则，一个模型相对于另一个模型的优点可能仅仅是由于参数的调整，并不能反映模型或特征的实际表现。</p>
<p>即使是最好的自动调整软件包仍然需要指定搜索的上限和下限，并且找到这些限制可能需要几次手动尝试。</p>
<p>在本例中，我们手动将逻辑正则化参数的搜索网格设置为{1e-5，0.001，0.1，1，10，100}。 上限和下限花费了几次尝试来缩小范围。 表4-1给出了每个特征集合的最优超参数设置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Method</th>
<th>L2 Regularization</th>
</tr>
</thead>
<tbody>
<tr>
<td>BOW</td>
<td>0.1</td>
</tr>
<tr>
<td>L2-normalized</td>
<td>10</td>
</tr>
<tr>
<td>TF-IDF</td>
<td>0.01</td>
</tr>
</tbody>
</table>
</div>
<p>我们也想测试tf-idf和BOW之间的精度差异是否是由于噪声造成的。 为此，我们使用k折交叉验证来模拟具有多个统计独立的数据集。它将数据集分为k个折叠。交叉验证过程通过分割后的数据进行迭代，使用除去某一折之外的所有内容进行训练，并用那一折验证结果。Scikit-Learn中的GridSearchCV功能通过交叉验证进行网格搜索。 下图显示了在每个特征集上训练的模型的精度测量分布箱线图。 盒子中线表示中位精度，盒子本身表示四分之一和四分之三分位之间的区域，而线则延伸到剩余的分布。</p>
<h3 id="通过重采样估计方差"><a href="#通过重采样估计方差" class="headerlink" title="通过重采样估计方差"></a>通过重采样估计方差</h3><p>现代统计方法假设底层数据是随机分布的。 数据导出模型的性能测量也受到随机噪声的影响。 在这种情况下，基于相似数据的数据集，不止一次进行测量总是比较好的。 这给了我们一个测量的置信区间。 K折交叉验证就是这样一种策略。 重采样是另一种从相同底层数据集生成多个小样本的技术。 </p>
<img src="/posts/5ffd1738/4.png">
<p>该图是分类器精度在每个特征集和正则化设置下的分布。 准确度是以5折交叉验证的平均准确度来衡量的</p>
<p>在图中，L2归一化后的特征结果看起来非常糟糕。 但不要被蒙蔽了 。准确率低是由于正则化参数设置不恰当造成的 - 实际证明次优超参数会得到相当错误的结论。 如果我们使用每个特征集的最佳超参数设置来训练模型，则不同特征集的测试精度非常接近。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Regularization Parameter</th>
<th>BOW</th>
<th>normalized</th>
<th>Tf-idf</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.00001</td>
<td>0.578971</td>
<td>0.575724</td>
<td>0.721638</td>
</tr>
<tr>
<td>0.001</td>
<td>0.751811</td>
<td>0.575724</td>
<td>0.788648*</td>
</tr>
<tr>
<td>0.1</td>
<td>0.782839*</td>
<td>0.589120</td>
<td>0.763566</td>
</tr>
<tr>
<td>1</td>
<td>0.773818</td>
<td>0.734247</td>
<td>0.741150</td>
</tr>
<tr>
<td>10</td>
<td>0.755160</td>
<td>0.776756*</td>
<td>0.721467</td>
</tr>
<tr>
<td>100</td>
<td>0.739373</td>
<td>0.761106</td>
<td>0.712309</td>
</tr>
</tbody>
</table>
</div>
<p>上面表中每个超参数设置的平均交叉验证分类器准确度。 星号表示最高精度</p>
<p>最终的训练和测试步骤来比较不同的特征集，得到下面的结果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Feature Set</th>
<th>Test Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bag-of-Words</td>
<td>0.78360708021</td>
</tr>
<tr>
<td>L2 -normalized</td>
<td>0.780178599904</td>
</tr>
<tr>
<td>Tf-Idf</td>
<td>0.788470738319</td>
</tr>
</tbody>
</table>
</div>
<p>适当的调整提高了所有特征集的准确性，并且所有特征集在正则化后进行逻辑回归得到了相近的准确率。tf-idf模型准确率略高，但这点差异可能没有统计学意义。 这些结果是完全神秘的。 如果特征缩放效果不如vanilla词袋的效果好，那为什么要这么做呢？ 如果tf-idf没有做任何事情，为什么总是要这么折腾？ 我们将在本章的其余部分中探索答案。</p>
<h2 id="深入：发生了什么？"><a href="#深入：发生了什么？" class="headerlink" title="深入：发生了什么？"></a>深入：发生了什么？</h2><p>为了明白结果背后隐含着什么，我们必须考虑模型是如何使用特征的。对于类似逻辑回归这种线性模型来说，是通过所谓的数据矩阵的中间对象来实现的。 数据矩阵包含以固定长度平面向量表示的数据点。 根据词袋向量，数据矩阵也被称为文档词汇矩阵。 图3-1显示了一个向量形式的词袋向量，图4-1显示了特征空间中的四个词袋向量。 要形成文档词汇矩阵，只需将文档向量取出，平放，然后将它们堆叠在一起。 这些列表示词汇表中所有可能的单词。 由于大多数文档只包含所有可能单词的一小部分，因此该矩阵中的大多数都是零，是一个稀疏矩阵。</p>
<img src="/posts/5ffd1738/5.png" title="包含五个文档7个单词的文档-词汇矩阵">
<p>特征缩放方法本质上是对数据矩阵的列操作。特别的，tf-idf和L2归一化都将整列（例如n-gram特征）乘上一个常数。</p>
<h2 id="Tf-idf-列缩放"><a href="#Tf-idf-列缩放" class="headerlink" title="Tf-idf  =  列缩放"></a>Tf-idf  =  列缩放</h2><p>Tf-idf和L2归一化都是数据矩阵上的列操作。训练线性分类器归结为寻找最佳的线性组合特征，这是数据矩阵的列向量。 解空间的特征是列空间和数据矩阵的空间。训练过的线性分类器的质量直接取决于数据矩阵的零空间和列空间。 大的列空间意味着特征之间几乎没有线性相关性，这通常是好的。 零空间包含“新”数据点，不能将其表示为现有数据的线性组合; 大的零空间可能会有问题。</p>
<p>列缩放操作如何影响数据矩阵的列空间和空间？ 答案是“不是很多”。但是在tf-idf和L2归一化之间有一个小小的差别。</p>
<p>由于几个原因，数据矩阵的零空间可能很大。 首先，许多数据集包含彼此非常相似的数据点。 这使得有效的行空间与数据集中数据的数量相比较小。 其次，特征的数量可以远大于数据的数量。 词袋特别擅长创造巨大的特征空间。 在我们的Yelp例子中，训练集中有29K条评论，但有47K条特征。 而且，不同单词的数量通常随着数据集中文档的数量而增长。 因此，添加更多的文档不一定会降低特征与数据比率或减少零空间。</p>
<p>在词袋模型中，与特征数量相比，列空间相对较小。 在相同的文档中可能会出现数目大致相同的词，相应的列向量几乎是线性相关的，这导致列空间不像它可能的那样满秩。 这就是所谓的秩亏。 </p>
<p>秩亏行空间和列空间导致模型空间预留过度的问题。 线性模型为数据集中的每个特征配置权重参数。 如果行和列空间满秩$^1$，那么该模型将允许我们在输出空间中生成任何目标向量。 当模型不满秩时，模型的自由度比需要的更大。 这使得找出解决方案变得更加棘手。</p>
<p>列空间被定义为所有列向量的线性组合：$a_{1} v_{1}+a_{2} v_{2}+\ldots+a_{n} v_{n}$。比方说，特征缩放用一个常数倍来替换一个列向量，$v_{1}=c v_{1}$。但是我们仍然可以通过用$\widetilde{a_{1}}=\frac{a_{1}}{c}$来 替换$a_{1}$，生成原始的线性组合。看起来，特征缩放不会改变列空间的秩。类似地，特征缩放不会影响空间的秩，因为可以通过反比例缩放权重向量中的对应条目来抵消缩放的特征列。</p>
<p>但是，仍然存在一个陷阱。 如果标量为0，则无法恢复原始线性组合;$v_{1}$消失了。 如果该向量与所有其他列线性无关，那么我们已经有效地缩小了列空间并放大了零空间。</p>
<p>如果该向量与目标输出不相关，那么这将有效地修剪掉噪声信号，这是一件好事。 这是tf-idf和L2归一化之间的关键区别。 L2归一化永远不会计算零的范数，除非该向量包含全零。 如果向量接近零，那么它的范数也接近于零。 按照小规范划分将突出向量并使其变大。</p>
<p>另一方面，如图2所示，Tf-idf可以生成接近零的缩放因子。 当这个词出现在训练集中的大量文档中时，会发生这种情况。 这样的话有可能与目标向量没有很强的相关性。 修剪它可以使模型专注于列空间中的其他方向并找到更好的解决方案。 准确度的提高可能不会很大，因为很少有噪声方向可以通过这种方式修剪。</p>
<p>在特征缩放的情况下，L2和tf-idf对于模型的收敛速度确实有促进。 这是该数据矩阵有一个更小的条件数的标志。 事实上，L2归一化使得条件数几乎一致。 但情况并非条件数越多，解决方案越好。 在这个实验中，L2归一化收敛比BOW或tf-idf快得多。 但它对过拟合也更敏感：它需要更多的正则化，并且对优化期间的迭代次数更敏感。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本章中，我们使用tf-idf作为入口点，详细分析特征变换如何影响（或不影响）模型。Tf-idf是特征缩放的一个例子，所以我们将它的性能与另一个特征缩放方法-L2标准化进行了对比。</p>
<p>结果并不如预期。Tf-idf和L2归一化不会提高最终分类器的准确度，而不会超出纯词袋。 在获得了一些统计建模和线性代数处理知识之后，我们意识到了为什么：<strong>他们都没有改变数据矩阵的列空间</strong>。</p>
<p>两者之间的一个小区别是，tf-idf可以“拉伸”字数以及“压缩”它。 换句话说，它使一些数字更大，其他数字更接近 归零。 因此，tf-idf可以完全消除无意义的单词。</p>
<p>我们还发现了另一个特征缩放效果：它改善了数据矩阵的条件数，使线性模型的训练速度更快。 L2标准化和tf-idf都有这种效果。</p>
<p>总而言之，正确的特征缩放可以有助于分类。 正确的缩放突出了信息性词语，并降低了常见单词的权重。 它还可以改善数据矩阵的条件数。 正确的缩放并不一定是统一的列缩放。</p>
<p>这个故事很好地说明了在一般情况下分析特征工程的影响的难度。 更改特征会影响训练过程和随后的模型。 线性模型是容易理解的模型。 然而，它仍然需要非常谨慎的实验方法和大量的深刻的数学知识来区分理论和实际的影响。 对于更复杂的模型或特征转换来说，这是不可能的。</p>
<h1 id="类别特征：机器鸡时代的鸡蛋计算"><a href="#类别特征：机器鸡时代的鸡蛋计算" class="headerlink" title="类别特征：机器鸡时代的鸡蛋计算"></a>类别特征：机器鸡时代的鸡蛋计算</h1><p>一个类别特征，见名思义，就是用来表达一种类别或标签。比如，一个类别特征能够表达世界上的主要城市，一年四季，或者说一个公司的产品(石油、路程、技术)。在真实世界的数据集中，类别值的数量总是有限的。同时这些值一般可以用数值来表示。但是，与其他数值变量不同，分类变量的值不能相互排序。(作为行业类型，石油与旅行无法进行比较)它们被称之为非序数的。</p>
<p>一个简单的问题可以作为测试是否应该是一个分类变量的试金石测试：“两个事情的值不同，或者它们类型不同，这有什么关系吗？”股价为500美元的比股价为100美元的价格高5倍。 所以股票价格应该用一个连续的数字变量表示。 另一方面，公司的产业（石油，旅游，技术等）应该无法被比较的，也就是类别特征。</p>
<p>大的分类变量在交易记录中特别常见。 对于实例中，许多Web服务使用id作为分类变量来跟踪用户具有数百至数百万的值，取决于唯一的数量服务的用户。 互联网交易的IP地址是另一个例子一个很大的分类变量。 它们是分类变量，因为即使用户ID和IP地址是数字，它们的大小通常与任务无关在眼前。 例如，在进行欺诈检测时，IP地址可能是相关的个人交易。 某些IP地址或子网可能会产生更多欺骗性交易比其他人。 但是164.203.x.x的子网本质上并不多欺诈性比164.202.x.x; 子网的数值无关紧要。</p>
<p>文档语料库的词汇可以被解释为一个大的分类变量，类别是唯一的单词。 它可能在计算上很昂贵代表如此多的不同类别。 如果一个类别（例如，单词）出现多个数据点（文档）中的时间，然后我们可以将它表示为一个计数并表示所有的类别通过他们的统计数字。 这被称为bin-counting。 我们用分类变量的共同表示开始讨论，并且最终蜿蜒曲折地讨论了大范围的bin-counting问题变量，这在现代数据集中非常普遍。</p>
<h2 id="对类别特征编码"><a href="#对类别特征编码" class="headerlink" title="对类别特征编码"></a>对类别特征编码</h2><p>分类变量的类别通常不是数字。例如，眼睛的颜色可以是“黑色”，“蓝色”，“棕色”等。因此，需要使用编码方法将这些非数字类别变为数字。 简单地将一个整数（比如1到k）分配给k个可能的类别中的每一个都是诱人的。 但是，由此产生的价值观可以互相授权，这在类别中不应该被允许。</p>
<h3 id="One-hot编码"><a href="#One-hot编码" class="headerlink" title="One-hot编码"></a>One-hot编码</h3><p>将类别特征进行表示一个最好的办法就是使用一组比特位来表达。每一位代表一个可能的类别。 如果该变量不能一次成为多个类别，那么该组中只有一位可以是1。 这被称为独热编码，它在Scikit Learn中实现<a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">sklearn.preprocessing.OneHotEncoder</a>。 每个位都是一个特征。 因此是一个绝对的具有k个可能类别的变量被编码为长度为k的特征向量</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>对3个城市的类别进行独热编码</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>City</td>
<td>e1</td>
<td>e2</td>
<td>e3</td>
</tr>
<tr>
<td>San Francisco</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>New York</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>Seattle</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>热编码非常易于理解。 但它使用的是比严格必要的更多的一点。 如果我们看到k-1位是零，那么最后一位必须是1，因为变量必须具有k个值中的一个。 在数学上，可以写下这个约束条件为“所有位的和必须等于1”。</p>
<script type="math/tex; mode=display">e1 + e2 + e3 + .... + ek =1</script><p>因此，我们有一个线性的依赖性。 线性相关特征，就像我们一样在tf-idf中发现，有点烦人，因为它意味着训练线性模型不会是唯一的。 特征的不同线性组合可以做出同样的预测，所以我们需要跳过额外条件的来理解特征对预测的影响。</p>
<h3 id="dummy编码"><a href="#dummy编码" class="headerlink" title="dummy编码"></a>dummy编码</h3><p>独热编码的问题是它允许k个自由度，其中变量本身只需要k-1。 虚拟编码通过仅使用表示中的k-1个特征来消除额外的自由度。 公共汽车下面有一个特征，由全零矢量表示。 这被称为参考类别。 虚拟编码和独热编码都是在Pandas中以<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html" target="_blank" rel="noopener">pandas.get_dummies</a>的形式实现的。</p>
<p>表 对3个城市的类别进行dummy编码</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">City</th>
<th style="text-align:center">e1</th>
<th style="text-align:center">e2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">San Francisco</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">New York</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">Seattle</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>使用虚拟编码进行建模的结果比单编码更易解释。这很容易在简单的线性回归问题中看到。 假设我们有一些数据关于三个城市的公寓租赁价格：旧金山，纽约和西雅图。（见表5-3）</p>
<p>表三个不同城市的公寓价格数据集</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th style="text-align:center">city</th>
<th style="text-align:center">Rent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">SF</td>
<td style="text-align:center">3999</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">SF</td>
<td style="text-align:center">4000</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">SF</td>
<td style="text-align:center">4001</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">NYC</td>
<td style="text-align:center">3499</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">NYC</td>
<td style="text-align:center">3500</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">NYC</td>
<td style="text-align:center">3501</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">Seattle</td>
<td style="text-align:center">2499</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">Seattle</td>
<td style="text-align:center">2500</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">Seattle</td>
<td style="text-align:center">2501</td>
</tr>
</tbody>
</table>
</div>
<img src="/posts/5ffd1738/6.jpg" title="公寓租金价格在one-hot编码中的向量空间表示。点的大小表达了数据集中租金不同价格的平均数">
<p>我们这时能够仅仅依靠城市这一个变量来建立线性回归来预测租金的价格。<br>线性回归模型可以这样写</p>
<script type="math/tex; mode=display">y=w1x1+w2x2+w3x3+...+wnxn</script><p>习惯上我们还添加一个常量来，这样的话当x全部为0，y不会为0.</p>
<p>通过独热编码，截距项表示目标变量的全局均值租金价格，并且每个线性系数表示该城市的平均租金与全局平均值的差异。</p>
<p>通过虚拟编码，偏差系数代表响应的平均值参考类别的变量y，在这个例子中是纽约市。该第i个特征的系数等于平均响应之间的差异第i类别的值和参考类别的平均值。</p>
<p>表：线性回归学得的系数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>id</th>
<th>x1</th>
<th>x2</th>
<th>x3</th>
<th>b</th>
</tr>
</thead>
<tbody>
<tr>
<td>one-hot</td>
<td>166.67</td>
<td>666.67</td>
<td>-833.33</td>
<td>3333.33</td>
</tr>
<tr>
<td>dummy coding</td>
<td>0</td>
<td>500</td>
<td>-1000</td>
<td>3500</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Effect-编码"><a href="#Effect-编码" class="headerlink" title="Effect 编码"></a>Effect 编码</h3><p>分类变量编码的另一种变体称为Effect编码。 Effect编码与虚拟编码非常相似，区别在于参考类别现在由所有-1的向量表示。</p>
<p>表: Effect编码表示3个城市</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>City</th>
<th>e1</th>
<th>e2</th>
</tr>
</thead>
<tbody>
<tr>
<td>San Francisco</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>New York</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Seattle</td>
<td>-1</td>
<td>-1</td>
</tr>
</tbody>
</table>
</div>
<p>Effect编码与虚拟编码非常相似，但是在线性回归中更容易被拟合。截距项表示目标的全球平均值变量，单个系数表示各个类别的平均值与全球平均值有多少差异。 （独热编码实际上具有相同的截距和系数，但在这种情况下，每个城市都有线性系数。 在效果编码中，没有单一特征代表参考类别。 因此，参考类别的影响需要分别计算为所有其他类别的系数的负和。</p>
<p>类别变量的优点和缺点</p>
<p>独热，虚拟和效果编码非常相似。 他们每个人都有优点和缺点。 独热编码是多余的，它允许多个有效模型一样的问题。 非唯一性有时候对解释有问题。该优点是每个特征都明显对应于一个类别。 此外，失踪数据可以编码为全零矢量，输出应该是整体目标变量的平均值。</p>
<p>虚拟编码和效果编码不是多余的。 他们产生独特和可解释的模型。 虚拟编码的缺点是它不能轻易处理缺少数据，因为全零矢量已经映射到参考类别。它还编码每个类别相对于参考类别的影响，其中看起来很奇怪。 效果编码通过使用不同的代码来避免此问题参考类别。 但是，所有-1的矢量都是一个密集的矢量，对于存储和计算来说都很昂贵。 因此，Pandas和Scikit Learn等流行的ML软件包选择了虚拟编码或独热编码，而不是效应编码。当类别数量变得非常多时，所有三种编码技术都会失效大。 需要不同的策略来处理非常大的分类变量。</p>
<h2 id="处理大量的类别特征"><a href="#处理大量的类别特征" class="headerlink" title="处理大量的类别特征"></a>处理大量的类别特征</h2><p>互联网上的自动数据收集可以生成大量的分类变量。这在诸如定向广告和欺诈检测等应用中很常见。 在有针对性的广告中，任务是根据用户的搜索查询或当前页面将用户与一组广告进行匹配。 功能包括用户ID，广告的网站域，搜索查询，当前页面以及这些功能的所有可能的成对连词。 （查询是一个文本字符串，可以切分成常用的文本特征，但查询通常很短，通常由短语组成，因此在这种情况下最好的行为通常是保持完整，或 通过哈希函数来简化存储和比较）其中每一个都是一个非常大的分类变量。 我们面临的挑战是如何找到一个能够提高内存效率的优秀特征表示，并生成训练速度快的准确模型。</p>
<p>对于这种类别特征处理的方案有：</p>
<ol>
<li>对编码不做任何事情。 使用便宜的训练简单模型。 在许多机器上将独热编码引入线性模型（逻辑回归或线性支持向量机）。</li>
<li>压缩编码，有两种方式 a. 对特征进行哈希—在线性回归中特别常见 b. bin-counting—在线性回归中与树模型都常见</li>
</ol>
<p>使用one-hot编码是可行的。在微软搜索广告研究中，Graepel等人 [2010]报告在贝叶斯概率回归模型中使用这种二值特征，可以使用简单更新在线进行培训。 与此同时，其他组织则争论压缩方法。 来自雅虎的研究人员 通过特征散列发誓[Weinberger et al。2009年]。 尽管McMahan等人[2013]在谷歌的广告引擎上尝试了功能哈希，并没有找到显着的改进。 然而，微软的其他人则被认为是计数[Bilenko，2015]。</p>
<p>所有这些想法都有利有弊。 </p>
<h2 id="特征哈希"><a href="#特征哈希" class="headerlink" title="特征哈希"></a>特征哈希</h2><p>散列函数是一个确定性函数，它映射一个潜在的无界整数到有限整数范围[1，m]。 由于输入域可能大于输出范围，多个数字可能会映射到相同的输出。 这被称为a碰撞。 统一的散列函数可确保大致相同数量的数字被映射到每个m箱。 在视觉上，我们可以将散列函数视为一台机器可以吸入编号的球并将它们传送到一个m箱。 球与相同的号码将始终被路由到同一个bin。</p>
<p>散列函数可以为任何可以用数字表示的对象构造（对于可以存储在计算机上的任何数据都是如此）：数字，字符串，复杂的结构等</p>
<img src="/posts/5ffd1738/7.jpg" title="哈希编码">
<p>当有很多特征时，存储特征向量可能占用很多空间。 特征散列将原始特征向量压缩为m维通过对特征ID应用散列函数来创建矢量。 例如，如果原件特征是文档中的单词，那么散列版本将具有固定的词汇大小为m，无论输入中有多少独特词汇。</p>
<p>功能散列的另一个变体添加了一个符号组件，因此计数也是从哈希箱中增加或减少。 这确保了内部产品之间散列特征与原始特征的期望值相同。</p>
<p>哈希后内积的值在时间复杂度在<code>O(1/(m**0.5))</code>.所以哈希表m的大小可以根据可接受的错误来选择。在实践中，选择合适的m可能需要一些试验和错误。特征哈希可以用于涉及特征内积的模型矢量和系数，例如线性模型和核心方法。 它一直证明在垃圾邮件过滤任务中取得成功[Weinberger等，2009]。在有针对性的广告案例中，McMahan et al。 [2013年]报告不能将预测误差降低到可接受的水平，除非m的数量级为数十亿。散列特征的一个缺点是散列特征是聚合的原始特征，不再可解释。</p>
<p>在这个例子中，我们将使用Yelp评论数据集来演示存储和,解释性使用的为sklearn的库<code>FeatureHasher</code>。在有针对性的广告案例中，McMahan</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">js = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'yelp_academic_dataset_review.json'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">        js.append(json.loads(f.readline()))</span><br><span class="line"></span><br><span class="line">review_df = pd.DataFrame(js)</span><br><span class="line"></span><br><span class="line">m = len(review_df.business_id.unique())</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;m</span><br><span class="line"><span class="number">4174</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: <span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> FeatureHasher</span><br><span class="line">   ...: </span><br><span class="line">   ...: h = FeatureHasher(n_features=m, input_type=<span class="string">'string'</span>)</span><br><span class="line">   ...: </span><br><span class="line">   ...: f = h.transform(review_df[<span class="string">'business_id'</span>])</span><br><span class="line">   ...: </span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: review_df[<span class="string">'business_id'</span>].unique().tolist()[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line">Out[<span class="number">5</span>]: </span><br><span class="line">[<span class="string">'9yKzy9PApeiPPOUJEtnvkg'</span>,</span><br><span class="line"> <span class="string">'ZRJwVLyzEJq1VAihDhYiow'</span>,</span><br><span class="line"> <span class="string">'6oRAC4uyJCsJl1X0WZpVSA'</span>,</span><br><span class="line"> <span class="string">'_1QQZuf4zZOyFCvXc0o6Vg'</span>,</span><br><span class="line"> <span class="string">'6ozycU1RpktNG2-1BroVtw'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: f.toarray()</span><br><span class="line">Out[<span class="number">6</span>]: </span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, ...,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, ...,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, ...,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       ..., </span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, ...,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, ...,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, ...,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<p>我们看看特征的存储</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Our pandas Series, in bytes: '</span>, getsizeof(review_df[<span class="string">'business_id'</span>]))</span><br><span class="line">print(<span class="string">'Our hashed numpy array, in bytes: '</span>, getsizeof(f))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;<span class="number">790104</span></span><br><span class="line">&gt;&gt;&gt;<span class="number">56</span></span><br></pre></td></tr></table></figure>
<p>我们可以清楚地看到如何使用特征散列会以计算方式使我们受益，牺牲直接的用户解释能力。 这是一个容易的权衡来接受何时从数据探索和可视化发展到机器学习管道对于大型数据集。</p>
<h2 id="bin-counting"><a href="#bin-counting" class="headerlink" title="bin-counting"></a>bin-counting</h2><p>Bin-counting是机器学习中常见的重新发现之一。 从广告点击率预测到硬件分支预测，它已经被重新创建并用于各种应用。 然而，因为它是一种特征工程技术，而不是一种建模或优化方法，所以没有关于该主题的研究论文。 </p>
<p>bin-counting的想法非常简单：不是使用分类变量的值作为特征，而是使用目标在该值下的条件概率。 换句话说，我们计算该值与我们希望预测的目标之间的关联统计信息，而不是编码的身份分类值。 对于那些熟悉朴素贝叶斯分类器的人来说，这个统计值应该敲响警钟，因为它是假设所有特性都是独立的类的条件概率。最好举例说明</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>User</td>
<td>Number of clicks</td>
<td>Number of non-clicks</td>
<td>probability of click</td>
<td>QueryHash,AdDomain</td>
<td>Number of clicks</td>
<td>Number of non-clicks</td>
<td>probability of click</td>
</tr>
<tr>
<td>Alice</td>
<td>5</td>
<td>120</td>
<td>0.0400</td>
<td>0x598fd4fe,foo.com</td>
<td>5000</td>
<td>30000</td>
<td>0.167</td>
</tr>
<tr>
<td>bob</td>
<td>20</td>
<td>230</td>
<td>0.0800</td>
<td>0x50fa3cc0,bar.org</td>
<td>100</td>
<td>900</td>
<td>0.100</td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>joe</td>
<td>2</td>
<td>3</td>
<td>0.400</td>
<td>0x437a45e1,qux.net</td>
<td>6</td>
<td>18</td>
<td>0.250</td>
</tr>
</tbody>
</table>
</div>
<p>Bin-counting假定历史数据可用于计算统计。 上表包含分类变量每个可能值的汇总历史计数。 根据用户点击任何广告的次数以及未点击的次数，我们可以计算用户“Alice”点击任何广告的概率。 同样，我们可以计算任何查询 - 广告 - 域组合的点击概率。 在训练时，每当我们看到“爱丽丝”时，都使用她的点击概率作为模型的输入特征。 QueryHash-AdDomain对也是如此，例如“0x437a45e1，qux.net”。</p>
<p>假设有10,000个用户。 独热编码会生成一个稀疏矢量长度为10,000，在列中对应于值的单个1当前数据点。 Bin-counting将所有10,000个二进制列编码为一个功能的真实值介于0和1之间。</p>
<p>除了历史点击概率外，我们还可以包含其他功能：原始计数本身（点击次数和非点击次数），对数比率或任何其他概率的衍生物。 我们的例子是预测广告点击率，通过率。 但该技术很容易应用于一般的二元分类。 它也可以使用通常的技术容易地扩展到多级分类将二元分类器扩展到多个类，即通过一对多优势比或其他多类标签编码。</p>
<h2 id="Bin-counting-的优势和对数比"><a href="#Bin-counting-的优势和对数比" class="headerlink" title="Bin-counting 的优势和对数比"></a>Bin-counting 的优势和对数比</h2><p>比值比通常定义在两个二元变量之间。 它通过提出这样一个问题来看待他们的联想强度：“当X为真时，Y有多大可能是真的”。例如，我们可能会问，“Alice点击广告的可能性大于 一般人口？“在这里，X是二进制变量”是Alice是当前用户“，而Y是变量”点击广告与否“。 该计算使用所谓的双向列联表（基本上，四个数字对应于X和Y的四种可能组合）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>click</th>
<th>Non-Click</th>
<th>Total</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Alice</td>
<td>5</td>
<td>120</td>
<td>125</td>
</tr>
<tr>
<td>Not Alice</td>
<td>995</td>
<td>18880</td>
<td>19875</td>
</tr>
<tr>
<td>Total</td>
<td>1000</td>
<td>19000</td>
<td>20000</td>
</tr>
</tbody>
</table>
</div>
<p>表：偶然发生的用户点击事件</p>
<p>定输入变量X和目标变量Y，优势比定义为</p>
<script type="math/tex; mode=display">
优势比=\frac{P(Y=1 | X=1) / P(Y=0 | X=1)}{P(Y=1 | X=0) / P(Y=0 | X=0)}</script><p>在我们的例子中，这意味着“爱丽丝点击广告而不是点击的可能性”和“其他人点击而非点击的可能性有多大”之间的比率。在这种情况下，数字是</p>
<script type="math/tex; mode=display">
odds　ratio(user,ad　click)=\frac{(5 / 125) /(120 / 125)}{(995 / 19,875) /(18,880 / 19,875)}=0.7906</script><p>更简单地说，我们可以看看分子，它检查多少可能性单个用户（Alice）是否点击广告而不是点击。 这适合大型具有许多值的分类变量，而不仅仅是两个。</p>
<script type="math/tex; mode=display">
odds　ratio(Alice,ad　click)=\frac{5 / 125}{120 / 125}=0.04166</script><p>概率比率可能很容易变得非常小或非常大。 （例如，将会有几乎不会点击广告的用户，也可能是点击广告的用户更频繁得多）日志转换再次来到我们的救援。 另一个对数的有用特性是它将一个划分变为一个减法。</p>
<script type="math/tex; mode=display">
log-odds　ratio(Alice,ad　click)=\log \left(\frac{5}{125}\right)-\log \left(\frac{120}{125}\right)=-3.178</script><p>简而言之，bin-counting将分类变量转换为有关的统计信息值。 它变成了一个大的，稀疏的分类变量的二进制表示变成一个非常小，密集的实值数值表示。</p>
<img src="/posts/5ffd1738/8.png" title="分类变量的一个热编码与箱计数统计的图示">
<p>在实施方面，垃圾箱计数需要在每个类别之间存储地图及其相关计数。 （其余的统计数据可以从中得到原始计数）。因此它需要O（k）空间，其中k是唯一值的数量的分类变量。</p>
<p>我们采用Kaggle的比赛<a href="https://www.kaggle.com/c/avazu-ctr-prediction" target="_blank" rel="noopener">Avazu</a>举个例子.</p>
<h3 id="Avazu-Click数据集"><a href="#Avazu-Click数据集" class="headerlink" title="Avazu Click数据集"></a>Avazu Click数据集</h3><ul>
<li>有24个变量，包括’点击’，一个二进制的点击、不点击计数器和’device_id’，用于跟踪显示广告的设备。</li>
<li>完整的数据集包含4,0428,967个观测值，其中有2,686,408个独特的设备。</li>
</ul>
<p>Avazu竞赛使用广告数据来预测点击率，但我们将使用它来演示如何bin计数可以大大减少大的特征空间流数据量。</p>
<p> Bin-counting例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_subset data is first 10K rows of 6+GB set</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df = pd.read_csv(<span class="string">'data/train_subset.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># How many unique features should we have after?</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(df[<span class="string">'device_id'</span>].unique())</span><br><span class="line"><span class="number">7201</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For each category, we want to calculate:</span></span><br><span class="line"><span class="comment"># Theta = [counts, p(click), p(no click), p(click)/p(no click)]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">click_counting</span><span class="params">(x, bin_column)</span>:</span></span><br><span class="line"><span class="meta">... </span>    clicks = pd.Series(x[x[<span class="string">'click'</span>] &gt; <span class="number">0</span>][bin_column].value_counts(), </span><br><span class="line"><span class="meta">... </span>                       name=<span class="string">'clicks'</span>)</span><br><span class="line"><span class="meta">... </span>    no_clicks = pd.Series(x[x[<span class="string">'click'</span>] &lt; <span class="number">1</span>][bin_column].value_counts(), </span><br><span class="line"><span class="meta">... </span>                          name=<span class="string">'no_clicks'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">... </span>    counts = pd.DataFrame([clicks,no_clicks]).T.fillna(<span class="string">'0'</span>)</span><br><span class="line"><span class="meta">... </span>    counts[<span class="string">'total_clicks'</span>] = counts[<span class="string">'clicks'</span>].astype(<span class="string">'int64'</span>) +</span><br><span class="line"><span class="meta">... </span>                             counts[<span class="string">'no_clicks'</span>].astype(<span class="string">'int64'</span>)</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> counts</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">bin_counting</span><span class="params">(counts)</span>:</span></span><br><span class="line"><span class="meta">... </span>    counts[<span class="string">'N+'</span>] = counts[<span class="string">'clicks'</span>]</span><br><span class="line"><span class="meta">... </span>                   .astype(<span class="string">'int64'</span>)</span><br><span class="line"><span class="meta">... </span>                   .divide(counts[<span class="string">'total_clicks'</span>].astype(<span class="string">'int64'</span>))</span><br><span class="line"><span class="meta">... </span>    counts[<span class="string">'N-'</span>] = counts[<span class="string">'no_clicks'</span>]</span><br><span class="line"><span class="meta">... </span>                   .astype(<span class="string">'int64'</span>)</span><br><span class="line"><span class="meta">... </span>                   .divide(counts[<span class="string">'total_clicks'</span>].astype(<span class="string">'int64'</span>))</span><br><span class="line"><span class="meta">... </span>    counts[<span class="string">'log_N+'</span>] = counts[<span class="string">'N+'</span>].divide(counts[<span class="string">'N-'</span>])</span><br><span class="line"><span class="meta">... </span>    <span class="comment"># If we wanted to only return bin-counting properties, </span></span><br><span class="line"><span class="meta">... </span>    <span class="comment"># we would filter here</span></span><br><span class="line"><span class="meta">... </span>    bin_counts = counts.filter(items= [<span class="string">'N+'</span>, <span class="string">'N-'</span>, <span class="string">'log_N+'</span>])</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> counts, bin_counts</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bin counts example: device_id</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bin_column = <span class="string">'device_id'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>device_clicks = click_counting(df.filter(items=[bin_column, <span class="string">'click'</span>]), </span><br><span class="line"><span class="meta">... </span>                               bin_column)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>device_all, device_bin_counts = bin_counting(device_clicks)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check to make sure we have all the devices</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(device_bin_counts)</span><br><span class="line"><span class="number">7201</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>device_all.sort_values(by = <span class="string">'total_clicks'</span>, ascending=<span class="keyword">False</span>).head(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">         clicks  no_clicks  total  N+        N-        log_N+</span><br><span class="line">a99f214a <span class="number">15729</span>   <span class="number">71206</span>      <span class="number">86935</span>  <span class="number">0.180928</span>  <span class="number">0.819072</span>  <span class="number">0.220894</span></span><br><span class="line">c357dbff <span class="number">33</span>      <span class="number">134</span>        <span class="number">167</span>    <span class="number">0.197605</span>  <span class="number">0.802395</span>  <span class="number">0.246269</span></span><br><span class="line"><span class="number">31</span>da1bd0 <span class="number">0</span>       <span class="number">62</span>         <span class="number">62</span>     <span class="number">0.000000</span>  <span class="number">1.000000</span>  <span class="number">0.000000</span></span><br><span class="line"><span class="number">936e92</span>fb <span class="number">5</span>       <span class="number">54</span>         <span class="number">59</span>     <span class="number">0.084746</span>  <span class="number">0.915254</span>  <span class="number">0.092593</span></span><br></pre></td></tr></table></figure>
<h2 id="关于稀有类别"><a href="#关于稀有类别" class="headerlink" title="关于稀有类别"></a>关于稀有类别</h2><p>就像罕见的词，罕见的类别需要特殊的处理。 想想一个用户每年登录一次：几乎没有数据可以可靠估计她广告的点击率。 而且，稀有类别会在计数表中浪费空间。解决这个问题的一种方法是通过补偿，一种积累的简单技术一个特殊垃圾箱中所有稀有类别的数量。 如果计数大于a一定的门槛，那么这个类别就有自己的统计数字。 否则，使用来自回退箱的统计数据。 这基本上会恢复单个的统计信息罕见类别与所有罕见类别的统计数据进行比较。 当使用back-off方法，它有助于为统计信息添加二进制指标来自后退箱。</p>
<img src="/posts/5ffd1738/9.png">
<p>图：如果一个罕见的类别获得计数，它可以使用自己的计数统计数据进行建模，从而超过退避箱的阈值。</p>
<p>还有另一种方法来处理这个问题，称为count-min sketch [Cormode和Muthukrishnan，2005]。 在这种方法中，所有类别，罕见或频繁类似通过多个散列函数进行映射，输出范围为m，远小于类别的数量，k。 当检索一个统计量时，计算所有的哈希值该类别，并返回最小的统计量。 拥有多个散列函数减轻单个散列函数内碰撞的可能性。 该计划有效因为可以做出散列函数次数m，散列表大小小于k，类别的数量，仍然保持较低的整体碰撞可能性。</p>
<p>下图说明。每个项i都映射到计数数组的每一行中的一个单元格。当$C_t$更新到它到达的项目$ i_t$时，$C_t$被添加到每个单元格中，使用$h_1…h_d$函数散列。</p>
<img src="/posts/5ffd1738/10.png" title="计数最小草图">
<h3 id="防止数据泄露"><a href="#防止数据泄露" class="headerlink" title="防止数据泄露"></a>防止数据泄露</h3><p>由于bin计数依赖于历史数据来生成必要的统计数据，因此它需要经过一段数据收集期等待，从而导致学习流程稍有延迟。此外，当数据分布发生变化时，需要更新计数。数据更改越快，重新计算计数的频率就越高。这对于目标广告等应用程序尤其重要，因为在目标广告中，用户偏好和流行的查询变化非常快，并且缺乏对当前发布的适应可能会给广告平台带来巨大损失。</p>
<p>有人可能会问，为什么不使用相同的数据集来计算相关统计量并训练模型？这个想法看起来很天真。这里最大的问题是统计数据涉及目标变量，这是模型试图预测的。使用输出来计算输入特征会导致一个称为泄漏的有害问题。简而言之，泄漏意味着信息被揭示给模型，从而使它有更好的预测的不切实际的优势。当测试数据泄露到训练集中，或者未来的数据泄漏到过去时，可能会发生这种情况。任何时候都会向模型提供在生产中实时进行预测时应该无法访问的信息，这会导致泄漏。 Kaggle的维基提供了更多泄漏示例以及为什么它对机器学习应用程序不利。</p>
<p> 如果bin计数程序使用当前数据点的标签来计算输入统计量的一部分，则这构成直接泄漏。防止这种情况的一种方法是在计数收集（用于计算箱计数统计）和训练之间进行严格分离，像下图所示，即使用较早批次的数据点进行计数，将当前数据点用于训练（将分类变量映射到历史统计我们刚刚收集），并使用未来的数据点进行测试。这解决了泄漏问题，但引入了上述延迟（输入统计信息，因此模型将跟踪当前数据）。</p>
<img src="/posts/5ffd1738/11.png" title="使用时间窗可防止Bin计数过程中的数据泄漏">
<p>事实证明，还有另一种基于差别隐私的解决方案。 如果统计数据的分布保持大致相同或不存在任何一个数据点，则该统计近似是防漏的。 在实践中，增加一个分布拉普拉斯（0,1）的小随机噪声足以掩盖单个数据点的任何潜在泄漏。 这个想法可以结合一次性计算来制定当前数据的统计数据。 Owen Zhang在他的“赢得数据科学竞赛”的演讲中详细介绍了这个技巧。</p>
<h2 id="COUNTS-WITHOUT-BOUNDS"><a href="#COUNTS-WITHOUT-BOUNDS" class="headerlink" title="COUNTS WITHOUT BOUNDS"></a>COUNTS WITHOUT BOUNDS</h2><p>如果在越来越多的历史数据下统计数据不断更新，原始数量将无限增长。这可能是模型的问题。训练有素的模型“知道”输入数据直至观察到的比例。一个训练有素的决策树可能会说“当x大于3时，预测为1”。一个经过训练的线性模型可能会说“乘以0.7的多个x并查看结果是否大于全局平均值”。这些可能是x介于0和5之间。但是除此之外会发生什么？没有人知道。 </p>
<p>当输入计数增加时，模型将需要重新训练以适应当前的比例。如果计数积累得相当缓慢，那么有效量表不会变得太快，并且模型不需要过于频繁地重新训练。但是当计数增加很快时，频繁的再培训将是一个麻烦。 </p>
<p>出于这个原因，使用标准化计数通常会更好 以已知的时间间隔为界。例如，估计的点击率是 有界[0，1]之间。另一种方法是采取对数变换，即施加一个 严格的限制，但是当数量非常大时，增加速度会很慢。</p>
<p> 这两种方法都不能防止转移投入分布，例如，去年的芭比娃娃现在已经过时，人们将不再点击这些广告。该模型需要重新训练以适应输入数据分布中的这些更根本性的变化，否则整个流程将需要迁移到模型不断适应输入的在线学习环境。</p>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>本章详细介绍的每种方法都有其优缺点。下面是权衡的结果。</p>
<h3 id="Plain-one-hot-encoding"><a href="#Plain-one-hot-encoding" class="headerlink" title="Plain one-hot encoding"></a>Plain one-hot encoding</h3><p>空间复杂度：o（n）使用稀疏向量格式，其中n是数据点的数目。</p>
<p>时间复杂度：o（nk）在线性模型下，其中K是类别的数目</p>
<p>优点:</p>
<ul>
<li>容易实现</li>
<li>更高的精度</li>
<li>在线学习特别容易扩展</li>
</ul>
<p>缺点</p>
<ul>
<li>计算不足</li>
<li>如果类别增加则不能够使用</li>
<li>对线性模型以外的任何其他方法都不可行</li>
<li>对于大数据集需要分布式训练</li>
</ul>
<h3 id="Feature-hashing"><a href="#Feature-hashing" class="headerlink" title="Feature hashing"></a>Feature hashing</h3><p>空间复杂度：o（n）使用稀疏矩阵格式，其中n是数据点的数量</p>
<p>时间复杂度：o（nm）在线性或内核模型下，其中m是哈希容器的数目。</p>
<p>优点:</p>
<ul>
<li>容易实现</li>
<li>容易训练</li>
<li>容易扩展到新类别</li>
<li>容易处理稀有类别</li>
<li>在线学习容易扩展</li>
</ul>
<p>缺点</p>
<ul>
<li>只能够使用线性或核模型</li>
<li>哈希编码很难解释</li>
<li>精度有争议</li>
</ul>
<h3 id="Bin-counting"><a href="#Bin-counting" class="headerlink" title="Bin-counting"></a>Bin-counting</h3><p>空间复杂度：o（n+k）表示每个数据点的小而密集的表示，加上必须为每个类别保留的计数统计信息。</p>
<p>时间复杂度：o（n）用于线性模型；也可用于非线性模型，如树</p>
<p>优点:</p>
<ul>
<li>训练快</li>
<li>能够使用树模型</li>
<li>容易扩展到新列类别</li>
<li>容易处理稀有类别</li>
<li>可解释</li>
</ul>
<p>缺点</p>
<ul>
<li>需要利用历史信息</li>
<li>对于在线学习有困难</li>
<li>会有数据泄露</li>
</ul>
<p>正如我们所看到的，这些方法都不完美。使用哪一个取决于所需的模型。线性模型的培训成本更低，因此可以处理非压缩表示，如一个热编码。另一方面，基于树的模型需要对所有特征进行重复搜索以获得正确的分割，因此仅限于小的表示，如bin计数。功能散列位于这两个极端之间，但是对于结果的准确性有着复杂的报告。</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Just for fun!</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Qsx 微信支付"/>
        <p>微信支付</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    

	<div>
		
			<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------<i class="fa fa-paw"></i> 文章已经到尾 <i class="fa fa-paw"></i>-------------</div>
    
</div>

		
	</div>
	
	
    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/算法/" rel="tag"># 算法</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/特征工程/" rel="tag"># 特征工程</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/8ae97bf8.html" rel="next" title="特征工程(上)">
                <i class="fa fa-chevron-left"></i> 特征工程(上)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="Qsx" />
            
              <p class="site-author-name" itemprop="name">Qsx</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Shelhon" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:ppp00qqq@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://weibo.com/qsxmybaby" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-weibo"></i>微博</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://zhihu.com/people/shelhon" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-globe"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.kofes.cn/" title="cz" target="_blank">cz</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#特征缩放的效果：从词袋到TF—IDF"><span class="nav-number">1.</span> <span class="nav-text">特征缩放的效果：从词袋到TF—IDF</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tf-Idf-词袋的小转折"><span class="nav-number">1.1.</span> <span class="nav-text">Tf-Idf:词袋的小转折</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tf-Idf的含义"><span class="nav-number">1.2.</span> <span class="nav-text">Tf-Idf的含义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#测试"><span class="nav-number">1.2.1.</span> <span class="nav-text">测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#首先建立分类数据集"><span class="nav-number">1.2.2.</span> <span class="nav-text">首先建立分类数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用tf-idf-转换缩放词袋"><span class="nav-number">1.2.3.</span> <span class="nav-text">用tf-idf 转换缩放词袋</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#测试集上进行特征缩放"><span class="nav-number">1.2.4.</span> <span class="nav-text">测试集上进行特征缩放</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用逻辑回归进行分类"><span class="nav-number">1.3.</span> <span class="nav-text">使用逻辑回归进行分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用正则化调整逻辑回归"><span class="nav-number">1.4.</span> <span class="nav-text">使用正则化调整逻辑回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#重点：比较模型时调整超参数"><span class="nav-number">1.5.</span> <span class="nav-text">重点：比较模型时调整超参数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#通过重采样估计方差"><span class="nav-number">1.5.1.</span> <span class="nav-text">通过重采样估计方差</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深入：发生了什么？"><span class="nav-number">1.6.</span> <span class="nav-text">深入：发生了什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tf-idf-列缩放"><span class="nav-number">1.7.</span> <span class="nav-text">Tf-idf  =  列缩放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">1.8.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#类别特征：机器鸡时代的鸡蛋计算"><span class="nav-number">2.</span> <span class="nav-text">类别特征：机器鸡时代的鸡蛋计算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#对类别特征编码"><span class="nav-number">2.1.</span> <span class="nav-text">对类别特征编码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#One-hot编码"><span class="nav-number">2.1.1.</span> <span class="nav-text">One-hot编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dummy编码"><span class="nav-number">2.1.2.</span> <span class="nav-text">dummy编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Effect-编码"><span class="nav-number">2.1.3.</span> <span class="nav-text">Effect 编码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#处理大量的类别特征"><span class="nav-number">2.2.</span> <span class="nav-text">处理大量的类别特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征哈希"><span class="nav-number">2.3.</span> <span class="nav-text">特征哈希</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bin-counting"><span class="nav-number">2.4.</span> <span class="nav-text">bin-counting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bin-counting-的优势和对数比"><span class="nav-number">2.5.</span> <span class="nav-text">Bin-counting 的优势和对数比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Avazu-Click数据集"><span class="nav-number">2.5.1.</span> <span class="nav-text">Avazu Click数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#关于稀有类别"><span class="nav-number">2.6.</span> <span class="nav-text">关于稀有类别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#防止数据泄露"><span class="nav-number">2.6.1.</span> <span class="nav-text">防止数据泄露</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#COUNTS-WITHOUT-BOUNDS"><span class="nav-number">2.7.</span> <span class="nav-text">COUNTS WITHOUT BOUNDS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结-1"><span class="nav-number">2.8.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Plain-one-hot-encoding"><span class="nav-number">2.8.1.</span> <span class="nav-text">Plain one-hot encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-hashing"><span class="nav-number">2.8.2.</span> <span class="nav-text">Feature hashing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bin-counting"><span class="nav-number">2.8.3.</span> <span class="nav-text">Bin-counting</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qsx</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">148.6k</span>
  
</div>










        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
</body>

</html>
